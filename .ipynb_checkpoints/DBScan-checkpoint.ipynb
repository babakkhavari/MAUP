{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba803e6c",
   "metadata": {},
   "source": [
    "# DBSCAN Notebook\n",
    "\n",
    "The following notebook can be used in order to replicate the population clusters developed for **The modifiable areal unit problem in geospatial least-cost electrification modelling** with the DBSCAN algorithm. Please refer **ADD LINK TO MENDELEY** for the final input files used in the analysis. \n",
    "\n",
    "The code outputs the clusters with population, id and area. Other attributes have to be added outside of this code\n",
    "\n",
    "## Theory\n",
    "\n",
    "We use the DBSCAN algorithm that comes in the scikit-learn package. More information [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). \n",
    "\n",
    "The algorithm takes several input, but for the case of this paper (and code) we alter two: min_sample and eps.\n",
    "\n",
    "* eps is a buffer which determines the maximum distance used for clustering purposes\n",
    "* min_sample determines the minumum number of points required in the buffer in order for the points to be clustered\n",
    "\n",
    "**Example:** Assume that you enter eps = 200 (m) and min_sample = 3. Then DBSCAN will cluster all points within a 200 m buffer if and only if there are 3 points within the buffer (including the original point itself). If the number of points within the buffer are less than 3 then they will remain unclustered and considered noise.\n",
    "\n",
    "\n",
    "## Input\n",
    "\n",
    "The algorithm makes use of two (2) GIS-datasets\n",
    "\n",
    "* **Population raster** - This raster can be resampled if you wish or it can be of native resolution. It will be used as basis to generate the clusters\n",
    "* **Population points** - Generated using the population raster. These points should include at least population count in the attribute table, but could also include additional columns.\n",
    "\n",
    "## Pre-processing \n",
    "\n",
    "The pre-processing is necessary to ensure that the algorithm works correctly. It can be done using any GIS software, for the paper we have used [QGIS](https://qgis.org/en/site/).\n",
    "\n",
    "* Please ensure that both of the datasets have the EPSG:4326 cooridnate (Warp in QGIS)\n",
    "* Please ensure that both datasets are clipped to the area of interest (Clip raster by mask layer in QGIS)\n",
    "* Please ensure that the population points are generated using the population raster (both datasets have to derive from the same source). This means if you resample the population raster you have to generate the points also with the resampled version of the raster. (Raster values to points after doing all of the raster processing (clipping and reprojecting)\n",
    "\n",
    "## Output\n",
    "\n",
    "If nothing is altered in the code 21 population bases will be produced with different eps (200-500 by steps of 50) and eps min_sample (1, 3 and 5). Each cluster will include three attributes\n",
    "\n",
    "1. id – The IDs are given as a unique number for each cluster. This enables the user to process the data contained in the clusters outside of a GIS software and then merge the data with the clusters.\n",
    "\n",
    "2. Population – This is the population in each cluster obtained from the population dataset.\n",
    "\n",
    "3. Area – The area of each cluster given in square kilometres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662fe87",
   "metadata": {},
   "source": [
    "## Cell 1 - Importing packages\n",
    "**Do not edit this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from osgeo import gdal, ogr, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio import features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bada88",
   "metadata": {},
   "source": [
    "## Cell 2 - Reading the input vector and raster populations. \n",
    "**Change this cell so that the paths lead to where you have the datasets saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"PATH_TO_POPULATION_POINTS\")\n",
    "raster = rio.open(\"PATH_TO_POPULATION_RASTER\")\n",
    "workspace = \"WHERE_YOU_WANT_EVERYTHING_TO_BE_SAVED\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05c7d7",
   "metadata": {},
   "source": [
    "## Cell 3 - Pre-processing\n",
    "\n",
    "Reprojects the layer to EPSG:3395. This is important due to eps in DBSCAN being measured in linear units. If you wish to use any other coordinate system go and select one from http://epsg.io/ (**You are recommended to keep 3395 however**)\n",
    "\n",
    "It also adds X and Y coordinates to the data and transforms it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b475ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = gdf.to_crs({ 'init': 'EPSG:3395'})\n",
    "pt[\"X\"] = pt[\"geometry\"].x\n",
    "pt[\"Y\"] = pt[\"geometry\"].y\n",
    "pt = pt[['X', 'Y']]\n",
    "numpis=pt.to_numpy()\n",
    "df = pd.DataFrame(numpis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619f94f",
   "metadata": {},
   "source": [
    "## Cell 4 - DBSCAN\n",
    "\n",
    "Conducts the DBSCAN clustering. The orignial code does this 21 times using 7 eps-values (given in list x) and 3 min_sample values (given in list y). This can be changed by either shortening or extending the lists. This steps takes some time.\n",
    "\n",
    "The output of this step are raster layer where each pixel has the value of the cluster that it is a part of.\n",
    "\n",
    "**Note** We wary with the x-list. This list describes the distance being used in DBSCAN and should be chosen with the cell size of the raster in mind. As an example we have used steps of 50 and a minumum value of 200 due to us haveing a cell size of 100 (steps less than 50 would not reach new cells and a starting vlaue less than 200 would equal 8-connectedness at most). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae54ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [500, 450, 400, 350, 300, 250, 200]\n",
    "y = [1, 3, 5]\n",
    "for core in y:\n",
    "    for val in x: \n",
    "        df = df.drop(columns=['geometry'], errors = 'ignore')\n",
    "        db = DBSCAN(eps=val, min_samples=core).fit(numpis)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "        df[\"clusters\"] = db.labels_\n",
    "        df[\"clusters\"].replace({-1: df[\"clusters\"].max()+1}, inplace=True)\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        df = df.rename(columns={0: \"X\",1:\"Y\"})\n",
    "\n",
    "        print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df, geometry=gpd.points_from_xy(df.X, df.Y))\n",
    "\n",
    "        gdf = gdf[[\"clusters\",\"geometry\"]]\n",
    "        gdf = gdf.rename(columns={'geom': 'geometry'})\n",
    "        gdf.crs = {'init' :'epsg:3395'}\n",
    "        gdf = gdf.to_crs({'init': 'EPSG:4326'})\n",
    "        gdf = gdf.rename(columns={'geometry': 'geom'})\n",
    "\n",
    "        dff = gdf[['clusters', 'geom']]\n",
    "        shapes = ((g, v) for v, g in zip(dff['clusters'].values, dff['geom'].values))\n",
    "\n",
    "        with rio.open(raster.name) as src:\n",
    "            image = features.rasterize(\n",
    "                        shapes,\n",
    "                        out_shape=src.shape,\n",
    "                        transform=src.transform,\n",
    "                        all_touched=False)\n",
    "            image = image.astype('float64')\n",
    "\n",
    "            out_meta = src.meta\n",
    "\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": src.height,\n",
    "                             \"width\": src.width,\n",
    "                             \"transform\": src.transform,\n",
    "                             'dtype': rio.float64,\n",
    "                             \"crs\": src.crs,\n",
    "                             \"compress\":'LZW',\n",
    "                             \"nodata\": 0})\n",
    "\n",
    "        with rio.open(\"clusters_\" +str(core) +'_'+ str(val) + \"_2.tif\", 'w', **out_meta) as dst:\n",
    "            dst.write(image, indexes=1)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a67f7",
   "metadata": {},
   "source": [
    "## Cell 5 - Converts the raster to polygons\n",
    "\n",
    "All cells that have received the same value in the cell ab ove are merged with one another using 8-connectedness. This step creates a single-part polygon (same cluster can give rise to several rows if they clusters are not connected by any geometries) **Please do not change anything in this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f823c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPolygon(Raster, output):\n",
    "   \n",
    "    Raster = gdal.Open(Raster)\n",
    "    \n",
    "    band = Raster.GetRasterBand(1)\n",
    "    bandArray = band.ReadAsArray()\n",
    "\n",
    "    outShapefile = output\n",
    "    \n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    if os.path.exists(outShapefile+\".shp\"):\n",
    "        driver.DeleteDataSource(outShapefile+\".shp\")\n",
    "    outDatasource = driver.CreateDataSource(outShapefile+ \".shp\")\n",
    "    \n",
    "    spat_ref = osr.SpatialReference()\n",
    "    proj = Raster.GetProjectionRef()\n",
    "    spat_ref.ImportFromWkt(proj)\n",
    "    \n",
    "    outLayer = outDatasource.CreateLayer(outShapefile+ \".shp\", srs=spat_ref)\n",
    "    newField = ogr.FieldDefn(\"clusters\", ogr.OFTInteger)\n",
    "    outLayer.CreateField(newField)\n",
    "    \n",
    "    gdal.Polygonize(band, band, outLayer, 0, [\"8CONNECTED=8\",\"GROUPBY=clusters\"], callback=None)\n",
    "    outDatasource.Destroy()\n",
    "    sourceRaster = None\n",
    "    \n",
    "for file in os.listdir(workspace):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".tif\"):\n",
    "        toPolygon(filename, filename[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c483dc",
   "metadata": {},
   "source": [
    "## Cell 6 - Collecting geometries\n",
    "\n",
    "Collecting the single-part polygons into multi-part. This cell also adds unique ids per clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(workspace):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        maximum = inFile[\"clusters\"].max()\n",
    "        multi = inFile.loc[inFile[\"clusters\"] < maximum]\n",
    "        dissolved = multi.dissolve(by=\"clusters\")\n",
    "        single = inFile.loc[inFile[\"clusters\"] == maximum]\n",
    "        \n",
    "        combined = gpd.GeoDataFrame(pd.concat([dissolved, single], ignore_index=True))\n",
    "        combined[\"id\"] = np.arange(len(combined))+1\n",
    "        \n",
    "        combined.to_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a08b7",
   "metadata": {},
   "source": [
    "## Cell 7 - Calculating areas of the clusters\n",
    "\n",
    "This is done with convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92204041",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(workspace):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        hull = inFile.dissolve(\"id\").convex_hull.reset_index().set_geometry(0)\n",
    "        reproj = hull.to_crs({ 'init': 'EPSG:3395'})\n",
    "        reproj[\"Area\"] = reproj.area/1000000\n",
    "        \n",
    "        \n",
    "        inFile = inFile[['id', 'geometry']]\n",
    "        joined = inFile.merge(reproj, on='id')\n",
    "        \n",
    "        joined_clean = joined[[\"id\",\"geometry\",\"Area\"]]\n",
    "        joined_clean[\"Country\"] = 'Benin'\n",
    "        joined_clean.to_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a11723",
   "metadata": {},
   "source": [
    "## Cell 8 -Adding population to the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(workspace):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        points_polys = gpd.sjoin(inFile, gdf, how=\"left\")\n",
    "        stats_pt  = points_polys.groupby('id_left').agg(\n",
    "        Pop  = ('Pop','sum'),\n",
    "        stats_pt.reset_index(inplace=True)\n",
    "        to_merge = stats_pt.rename(columns={'id_left': \"id\"})\n",
    "        \n",
    "        joined = inFile.merge(to_merge, on='id')\n",
    "        joined.to_file(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
