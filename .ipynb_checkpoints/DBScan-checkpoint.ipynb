{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76ce565",
   "metadata": {},
   "source": [
    "# DBScan\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "\n",
    "Inputs that are used: <br/>\n",
    "**eps** = Negihbourhood size will do in intervalls of 50 from 100 to 500<br/>\n",
    "**min_sample** = Minimum points in radius (including centerpoint) in order for centerpoint to be considered a core point. In this notebook I loop through 1, 3 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from osgeo import gdal, ogr, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(r\"C:\\PhD\\Papers\\2. MAUP\\Namibia\\Points_for_input.shp\")\n",
    "#raster = rio.open(r\"C:\\PhD\\Papers\\2. MAUP\\Benin\\clusters\\pop\\pop100m.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b475ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = gdf.to_crs({ 'init': 'EPSG:3395'})\n",
    "pt[\"X\"] = pt[\"geometry\"].x\n",
    "pt[\"Y\"] = pt[\"geometry\"].y\n",
    "pt = pt[['X', 'Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpis=pt.to_numpy()\n",
    "df = pd.DataFrame(numpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae54ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DBScan with 7 different buffers and 3 different cores\n",
    "#Converts points to raster\n",
    "x = [500, 450, 400, 350, 300, 250, 200]\n",
    "y = [1, 3, 5]\n",
    "for core in y:\n",
    "    for val in x: \n",
    "        df = df.drop(columns=['geometry'], errors = 'ignore')\n",
    "        db = DBSCAN(eps=val, min_samples=core).fit(numpis)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "        df[\"clusters\"] = db.labels_\n",
    "        df[\"clusters\"].replace({-1: df[\"clusters\"].max()+1}, inplace=True)\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        df = df.rename(columns={0: \"X\",1:\"Y\"})\n",
    "\n",
    "        print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df, geometry=gpd.points_from_xy(df.X, df.Y))\n",
    "\n",
    "        gdf = gdf[[\"clusters\",\"geometry\"]]\n",
    "        gdf = gdf.rename(columns={'geom': 'geometry'})\n",
    "        gdf.crs = {'init' :'epsg:3395'}\n",
    "        gdf = gdf.to_crs({'init': 'EPSG:4326'})\n",
    "        gdf = gdf.rename(columns={'geometry': 'geom'})\n",
    "\n",
    "        dff = gdf[['clusters', 'geom']]\n",
    "        shapes = ((g, v) for v, g in zip(dff['clusters'].values, dff['geom'].values))\n",
    "\n",
    "        with rio.open(raster.name) as src:\n",
    "            image = features.rasterize(\n",
    "                        shapes,\n",
    "                        out_shape=src.shape,\n",
    "                        transform=src.transform,\n",
    "                        all_touched=False)\n",
    "            image = image.astype('float64')\n",
    "\n",
    "            out_meta = src.meta\n",
    "\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": src.height,\n",
    "                             \"width\": src.width,\n",
    "                             \"transform\": src.transform,\n",
    "                             'dtype': rio.float64,\n",
    "                             \"crs\": src.crs,\n",
    "                             \"compress\":'LZW',\n",
    "                             \"nodata\": 0})\n",
    "\n",
    "        with rio.open(\"clusters_\" +str(core) +'_'+ str(val) + \"_2.tif\", 'w', **out_meta) as dst:\n",
    "            dst.write(image, indexes=1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f823c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPolygon(Raster, output):\n",
    "   \n",
    "    Raster = gdal.Open(Raster)\n",
    "    \n",
    "    band = Raster.GetRasterBand(1)\n",
    "    bandArray = band.ReadAsArray()\n",
    "\n",
    "    outShapefile = output\n",
    "    \n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    if os.path.exists(outShapefile+\".shp\"):\n",
    "        driver.DeleteDataSource(outShapefile+\".shp\")\n",
    "    outDatasource = driver.CreateDataSource(outShapefile+ \".shp\")\n",
    "    \n",
    "    spat_ref = osr.SpatialReference()\n",
    "    proj = Raster.GetProjectionRef()\n",
    "    spat_ref.ImportFromWkt(proj)\n",
    "    \n",
    "    outLayer = outDatasource.CreateLayer(outShapefile+ \".shp\", srs=spat_ref)\n",
    "    newField = ogr.FieldDefn(\"clusters\", ogr.OFTInteger)\n",
    "    outLayer.CreateField(newField)\n",
    "    \n",
    "    gdal.Polygonize(band, band, outLayer, 0, [\"8CONNECTED=8\",\"GROUPBY=clusters\"], callback=None)\n",
    "    outDatasource.Destroy()\n",
    "    sourceRaster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polygonize rasters\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".tif\"):\n",
    "        toPolygon(filename, filename[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting geometries and adds id\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        maximum = inFile[\"clusters\"].max()\n",
    "        multi = inFile.loc[inFile[\"clusters\"] < maximum]\n",
    "        dissolved = multi.dissolve(by=\"clusters\")\n",
    "        single = inFile.loc[inFile[\"clusters\"] == maximum]\n",
    "        \n",
    "        combined = gpd.GeoDataFrame(pd.concat([dissolved, single], ignore_index=True))\n",
    "        combined[\"id\"] = np.arange(len(combined))+1\n",
    "        \n",
    "        combined.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92204041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates area (convex hull)\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        hull = inFile.dissolve(\"id\").convex_hull.reset_index().set_geometry(0)\n",
    "        reproj = hull.to_crs({ 'init': 'EPSG:3395'})\n",
    "        reproj[\"Area\"] = reproj.area/1000000\n",
    "        \n",
    "        \n",
    "        inFile = inFile[['id', 'geometry']]\n",
    "        joined = inFile.merge(reproj, on='id')\n",
    "        \n",
    "        joined_clean = joined[[\"id\",\"geometry\",\"Area\"]]\n",
    "        joined_clean[\"Country\"] = 'Benin'\n",
    "        joined_clean.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding population, ElecPop and NTL\n",
    "\n",
    "gdf = gpd.read_file(r\"C:\\PhD\\Papers\\2. MAUP\\Benin\\Points.shp\")\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        points_polys = gpd.sjoin(inFile, gdf, how=\"left\")\n",
    "        stats_pt  = points_polys.groupby('id_left').agg(\n",
    "        Pop  = ('Pop','sum'),\n",
    "        ElecPop  = ('ElecPop','sum'),\n",
    "        Nightlight  = ('NightLight','max'))\n",
    "        stats_pt.reset_index(inplace=True)\n",
    "        to_merge = stats_pt.rename(columns={'id_left': \"id\"})\n",
    "        \n",
    "        joined = inFile.merge(to_merge, on='id')\n",
    "        joined.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7340e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizing\n",
    "df = pd.DataFrame(columns=['Country', 'Buffer', 'Core','Area','PopDensity','MaxPop','AveragePop', 'Nrs', 'nonClustered'])\n",
    "buffer = 200\n",
    "core = 1\n",
    "i = 0\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        subset_df = inFile[inFile[\"Area\"] < 0.01]\n",
    "\n",
    "        column_count = subset_df.count()\n",
    "        \n",
    "        df.loc[i] = ['Namibia'] + [buffer] + [core] + [inFile[\"Area\"].sum()] + [inFile[\"Pop\"].sum()/inFile[\"Area\"].sum()] + [inFile[\"Pop\"].max()] + [inFile[\"Pop\"].mean()] + [len(inFile.index)] + [column_count[1]]\n",
    "\n",
    "        if buffer < 500:\n",
    "            buffer = buffer + 50\n",
    "            core = core\n",
    "        else:\n",
    "            buffer = 200\n",
    "            core = core + 2\n",
    "        \n",
    "        i = i + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding inputs\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\OnSTOVE\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile = gpd.read_file(filename)\n",
    "        \n",
    "        hull = inFile.dissolve(\"id\").convex_hull.reset_index().set_geometry(0)\n",
    "        hull = hull.rename(columns={0: \"geometry\"})\n",
    "        convexhull=gpd.GeoDataFrame(data=hull, columns=[\"id\",\"geometry\"])\n",
    "        \n",
    "        points_polys = gpd.sjoin(convexhull, gdf, how=\"left\")\n",
    "        \n",
    "        stats_pt  = points_polys.groupby('id').agg(\n",
    "        WindVel  = ('WindVel','mean'),\n",
    "        GHI  = ('GHI','mean'),\n",
    "        TravelHours  = ('TravelHour','min'),\n",
    "        Elevation  = ('Elevation','mean'),\n",
    "        ResidentialDemandTierCustom  = ('Residentia','mean'),\n",
    "        SubstationDist  = ('Substation','min'),\n",
    "        CurrentHVLineDist  = ('CurrentHVL','min'),\n",
    "        CurrentMVLineDist  = ('CurrentMVL','min'),\n",
    "        RoadDist  = ('RoadDist','min'),\n",
    "        TransformerDist  = ('Transforme','min'),\n",
    "        PlannedHVLineDist  = ('CurrentMVL','min'),\n",
    "        PlannedMVLineDist  = ('PlannedMVL','min'),\n",
    "        HydropowerDist  = ('Hydropower','min'),\n",
    "        Hydropower  = ('Hydropow_1','min'),\n",
    "        HydropowerFID  = ('Hydropow_2','min'))\n",
    "        \n",
    "        stats_pt.reset_index(inplace=True)\n",
    "        joined = inFile.merge(stats_pt, on='id')\n",
    "        \n",
    "        points_polys = gpd.sjoin(gdf, convexhull, how=\"left\")\n",
    "        stats_pt = points_polys[points_polys[\"id\"] >= 0]\n",
    "        stats_pt = stats_pt.groupby(['id', 'LandCover']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "        stats_pt = stats_pt.loc[stats_pt.groupby('id')['count'].idxmax()]\n",
    "        joined = joined.merge(stats_pt, on='id')\n",
    "        \n",
    "        points_polys = gpd.sjoin(gdf, convexhull, how=\"left\")\n",
    "        stats_pt = points_polys[points_polys[\"id\"] >= 0]\n",
    "        stats_pt = stats_pt.groupby(['id', 'Slope']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "        stats_pt = stats_pt.loc[stats_pt.groupby('id')['count'].idxmax()]\n",
    "        clusters = joined.merge(stats_pt, on='id')\n",
    "        \n",
    "        clusters[\"IsUrban\"] = 0  \n",
    "        clusters[\"PerCapitaDemand\"] = 0\n",
    "        clusters[\"PerCapitaDemand\"] = 0\n",
    "        clusters[\"HealthDemand\"] = 0     \n",
    "        clusters[\"EducationDemand\"] = 0     \n",
    "        clusters[\"AgriDemand\"] = 0  \n",
    "        clusters[\"CommercialDemand\"] = 0\n",
    "        clusters[\"Conflict\"] = 0       \n",
    "        clusters[\"ElectrificationOrder\"] = 0\n",
    "        clusters[\"ResidentialDemandTier1\"] = 7.74\n",
    "        clusters[\"ResidentialDemandTier2\"] = 43.8\n",
    "        clusters[\"ResidentialDemandTier3\"] = 160.6\n",
    "        clusters[\"ResidentialDemandTier4\"] = 423.4\n",
    "        clusters[\"ResidentialDemandTier5\"] = 598.6\n",
    " \n",
    "        clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "        clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "        \n",
    "        del clusters['count_x']\n",
    "        del clusters['count_y']\n",
    "        \n",
    "        df1 = pd.DataFrame(clusters.drop(columns='geometry'))\n",
    "        df1.to_csv(filename[:-3]+'csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f1afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khavari\\.conda\\envs\\MAUP\\lib\\site-packages\\geopandas\\geodataframe.py:577: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    }
   ],
   "source": [
    "urb_clus = gpd.read_file(r\"C:\\PhD\\Papers\\2. MAUP\\Malawi\\urb_clus.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Urban\n",
    "#urb_clus = gpd.read_file(r\"C:\\PhD\\Papers\\2. MAUP\\Benin\\Urb_clus.shp\")\n",
    "\n",
    "urb_clus_csv = urb_clus.rename(columns={'Population': \"Pop\"})\n",
    "urb_clus_csv = urb_clus_csv.rename(columns={'NightLight': \"Nightlight\", 'TravelHour': \"TravelHours\", 'Residentia': \"ResidentialDemandTierCustom\", \n",
    "                                            'Substation': \"SubstationDist\", 'CurrentHVL': \"CurrentHVLineDist\", 'CurrentMVL': \"CurrentMVLineDist\",\n",
    "                                            'PlannedHVL': \"PlannedHVLineDist\",'PlannedMVL': \"PlannedMVLineDist\",'Transforme': \"TransformerDist\", \n",
    "                                            'Hydropower': \"HydropowerDist\", 'Hydropow_1': \"Hydropower\", 'Hydropow_2': \"HydropowerFID\", \n",
    "                                            'PerCapitaD': \"PerCapitaDemand\", 'HealthDema': \"HealthDemand\", 'EducationD': \"EducationDemand\", \n",
    "                                            'Electrific': \"ElectrificationOrder\", 'Commercial': \"CommercialDemand\", \n",
    "                                            'Resident_1': \"ResidentialDemandTier1\", 'Resident_2': \"ResidentialDemandTier2\", \n",
    "                                            'Resident_3': \"ResidentialDemandTier3\", 'Resident_4': \"ResidentialDemandTier4\",\n",
    "                                            'Resident_5': \"ResidentialDemandTier5\"})\n",
    "urb_cluster = urb_clus[[\"id\", \"Area\", \"Country\", \"Population\", \"ElecPop\", \"NightLight\", \"geometry\"]]\n",
    "urb_cluster = urb_cluster.rename(columns={'Population': \"Pop\", \"NightLight\":\"Nightlight\"})\n",
    "\n",
    "for file in os.listdir(r\"C:\\OnSSET\\OnSSET_GIS_Extraction_notebook\\MAUP\"):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".shp\"):\n",
    "        inFile_shp = gpd.read_file(filename)\n",
    "        inFile_csv = gpd.read_file(filename[:-3]+\"csv\")\n",
    "        \n",
    "        max_id = inFile_shp[\"id\"].max()\n",
    "        urb_cluster[\"id2\"] = np.arange(len(urb_cluster))+max_id+1\n",
    "        id_column = urb_cluster[[\"id\",\"id2\"]]\n",
    "        urb_clus_csv = urb_clus_csv.merge(id_column, on='id')\n",
    "        urb_clus_csv[\"id\"] = urb_clus_csv[\"id2\"]\n",
    "        urb_cluster[\"id\"] = urb_cluster[\"id2\"]\n",
    "        del urb_cluster[\"id2\"]\n",
    "        del urb_clus_csv[\"id2\"]\n",
    "\n",
    "        urb_file = pd.DataFrame(urb_clus_csv.drop(columns='geometry'))\n",
    "        \n",
    "        rdf_shp = gpd.GeoDataFrame(pd.concat([inFile_shp, urb_cluster], ignore_index=True))\n",
    "        \n",
    "        rdf_csv = pd.concat([inFile_csv, urb_clus_csv])\n",
    "        del rdf_csv[\"geometry\"]\n",
    "        \n",
    "        rdf_shp.to_file(filename)\n",
    "        rdf_csv.to_csv(filename[:-3]+\"csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc2e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
